{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the daily NOK summary for each sensor and the daily DCU offline duration\n",
    "------------------\n",
    "\n",
    "<span style=\"color:red\">NOTE: the cells has to be sequential executed </span>.\n",
    "\n",
    "Folder Structure:\n",
    "1. /Data\n",
    " + the original folder from sftp. \n",
    "2. /Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dcu_offline_interval = 60*30 #define the dcu offline interval as 30 minutes\n",
    "\n",
    "sensorList = [\"door_contact_as\",\"living_room_as\",\"bedroom_as\",\"bed_as\",\"bathroom_as\",\"kitchen_as\",\"readingroom_as\",\"spareroom_as\"]\n",
    "\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "def seperateCrossDayRecord(df):\n",
    "    df_new = pd.DataFrame()\n",
    "    for index,row in df.iterrows():\n",
    "        #if the dcu offline interval cross date, seperate it to multiple record\n",
    "        if row.date.date() != row.next_date.date():\n",
    "            date_temp = row.date\n",
    "            while date_temp.date() < row.next_date.date():\n",
    "                date_temp_next = datetime.datetime.combine(date_temp + datetime.timedelta(days=1),datetime.datetime.min.time())\n",
    "                #print(date_temp,date_temp_next)\n",
    "                delta = date_temp_next - date_temp\n",
    "                #print()\n",
    "                #print(delta)\n",
    "                delta_s = delta.total_seconds()\n",
    "                #delta_s = delta/np.timedelta64(1,'s')\n",
    "                #print({'date':date_temp,\"next_date\":date_temp_next,\"delta\":delta,'delta_s':delta_s})\n",
    "                df_new = df_new.append({'date':date_temp,'next_date':date_temp_next,'delta':delta,'delta_s':delta_s},ignore_index=True )\n",
    "                date_temp = date_temp_next\n",
    "                #print(df_new)\n",
    "            delta = row.next_date - date_temp\n",
    "            delta_s = delta.total_seconds()\n",
    "            df_new = df_new.append({'date':date_temp,'next_date':row.next_date,'delta':delta,'delta_s':delta_s},ignore_index=True )\n",
    "        else:\n",
    "            #print(row)\n",
    "            df_new = df_new.append(row,ignore_index=True)\n",
    "\n",
    "    return df_new\n",
    "\n",
    "def merge_OfflineDuration_into_NokSummary(df_offline,df_nok):\n",
    "    #df_nok_new = pf.DataFrame()\n",
    "    flag = False # false mean the nok records doesn't contain the offline record. should insert into\n",
    "    for index_offline, row_offline in df_offline.iterrows():\n",
    "        for index_nok,row_nok in df_nok.iterrows():\n",
    "            if row_nok.date<=row_offline.date and row_nok.next_date >= row_offline.next_date:\n",
    "                flag = True\n",
    "                break\n",
    "        if flag==False:\n",
    "            df_nok = df_nok.append(row_offline,ignore_index=True)\n",
    "    return df_nok\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_file(file_list):\n",
    "    time_start = time.time()\n",
    "    df_from_each_file = (pd.read_csv(f,parse_dates=[1]) for f in file_list)\n",
    "    df   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "    df = df.sort_values(['date'])\n",
    "\n",
    "    print(\"\\tLoad File Cost: \", round(time.time() - time_start, 3), \"seconds\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Proess Offline Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_offline(df):\n",
    "    time_start = time.time()\n",
    "\n",
    "    #print(df.head())\n",
    "    df_offline = pd.concat([df['date']],axis=1, keys=['date'])\n",
    "    #reformat the dataframe, add a new colume \"nexta_date\"\n",
    "    df_offline['next_date']=(df_offline['date'].shift(periods=-1)).fillna(df_offline['date'])\n",
    "    #caculate the time gap between nearby records\n",
    "    df_offline['delta'] = (df_offline['next_date'] - df_offline['date']).fillna(0)\n",
    "    # chagne delta time duration to seconds\n",
    "    print(\"1: \", round(time.time() - time_start, 3), \"seconds\")\n",
    "    df_offline['delta_s']= df_offline['delta'].apply(lambda x: x.total_seconds()).astype('int64')\n",
    "    # filter the records more then the pre-defined offline interval\n",
    "    df_offline = df_offline[df_offline['delta_s']>dcu_offline_interval]\n",
    "    #print(df_offline)\n",
    "    print(\"2: \", round(time.time() - time_start, 3), \"seconds\")\n",
    "    #divide the records which corssing day\n",
    "    df_offline_divided = seperateCrossDayRecord(df_offline)\n",
    "    df_offline_divided= df_offline_divided[['date', 'next_date', 'delta', 'delta_s']]\n",
    "    print(\"3: \", round(time.time() - time_start, 3), \"seconds\")\n",
    "    #print(df_offline_divided)\n",
    "    #sum the daily offline duration.\n",
    "    df_offline_divided_day_sum = df_offline_divided.groupby(df_offline_divided['date'].map(lambda x: x.date())).sum().reset_index()\n",
    "    print(\"4: \", round(time.time() - time_start, 3), \"seconds\")\n",
    "    #df_offline_divided_day_sum= df_offline_divided_day_sum[['date','delta_s']]\n",
    "    df_offline_divided_day_sum.columns = ['day', 'dcu_offline_duration']\n",
    "    df_offline_divided_day_sum = df_offline_divided_day_sum.set_index(['day'])\n",
    "\n",
    "    #reindex the df\n",
    "    df_offline_divided_day_sum = df_offline_divided_day_sum.reindex(pd.date_range(df['date'].min().date(),df['date'].max().date()), fill_value=0)\n",
    "    df_offline_divided_day_sum.head()\n",
    "    print(\"5: \", round(time.time() - time_start, 3), \"seconds\")\n",
    "    df_final = df_offline_divided_day_sum\n",
    "    print(\"\\tOffline Process Cost: \", round(time.time() - time_start, 3), \"seconds\")\n",
    "    df_final.head()\n",
    "    return df_offline,df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Process NOK: each sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def proess_nok(df,df_offline,df_final):\n",
    "    for sensor_name in sensorList:\n",
    "        time_start = time.time()\n",
    "        #deal with NOK door\n",
    "\n",
    "        #replace the \"Yes\" & \"No\" with \"OK\"\n",
    "\n",
    "        df_sensor = pd.concat([df['date'],df[sensor_name]],axis=1, keys=['date', 'sensor_status'])\n",
    "        #print(df_sensor)\n",
    "        df_sensor = df_sensor.dropna(subset=['sensor_status'])\n",
    "        if df_sensor.empty ==True:\n",
    "            df_final['nok_'+sensor_name[:-3]]=''\n",
    "        else:    \n",
    "            df_sensor[\"sensor_status\"].replace(['No', 'Yes'], ['OK', 'OK'], inplace=True)\n",
    "            #print(df_sensor)\n",
    "            # Drop consecutive duplicates status \n",
    "            df_sensor = df_sensor.loc[df_sensor['sensor_status'].shift() != df_sensor['sensor_status']]\n",
    "\n",
    "            #print(\"\\n=====After drop the consecutive duplicate status========\")\n",
    "            #print(df_sensor)\n",
    "\n",
    "            #print(\"\\n=====Caculate the distance for each status change========\")\n",
    "            df_sensor['next_date']=(df_sensor['date'].shift(periods=-1)).fillna(df_sensor['date'])\n",
    "            # when we drop the consecutive duplicated status,we just keep the first duplicated record.\n",
    "            #*******************************************************************************#\n",
    "            # if the last record is NOK, it means the NOK status continue to the last raw record.\n",
    "            if df_sensor.iloc[-1].sensor_status == 'NOK':\n",
    "                df_sensor.set_value(df_sensor.index[-1],'next_date',df.iloc[-1].date)\n",
    "            #*******************************************************************************#    \n",
    "            #caculate the time gap between nearby records\n",
    "            df_sensor['delta'] = (df_sensor['next_date'] - df_sensor['date']).fillna(0)\n",
    "            df_sensor['delta_s']= df_sensor['delta'].apply(lambda x: x.total_seconds()).astype('int64')\n",
    "            #print(df_sensor)\n",
    "\n",
    "            #print(\"\\n=====Filter the NOK status========\")\n",
    "            df_sensor_nok = df_sensor[df_sensor['sensor_status']=='NOK']\n",
    "            #print(df_sensor_nok)\n",
    "            df_sensor_nok = pd.concat([df_sensor_nok['date'],df_sensor_nok['next_date'],df_sensor_nok['delta'],df_sensor_nok['delta_s']],axis=1)\n",
    "\n",
    "\n",
    "            #print(\"\\n=====Merge with offline status========\")\n",
    "            df_sensor_nok = merge_OfflineDuration_into_NokSummary(df_offline,df_sensor_nok)\n",
    "            #print(df_sensor_nok)\n",
    "            df_sensor_nok = df_sensor_nok.sort_values(['date'])\n",
    "            #print(df_sensor_nok)\n",
    "            #print(df_sensor_nok.head())\n",
    "\n",
    "\n",
    "\n",
    "            #divide the records which corssing day\n",
    "            #print(df_sensor_nok)\n",
    "            df_sensor_nok_divided = seperateCrossDayRecord(df_sensor_nok)\n",
    "            df_sensor_nok_divided= df_sensor_nok_divided[['date', 'next_date', 'delta', 'delta_s']]\n",
    "            #print(df_sensor_nok_divided.head())\n",
    "\n",
    "            #print(df_sensor_nok_divided)\n",
    "            #sum the daily offline duration.\n",
    "            df_sensor_nok_divided_day_sum = df_sensor_nok_divided.groupby(df_sensor_nok_divided['date'].map(lambda x: x.date())).sum().reset_index()\n",
    "            #print(df_sensor_nok_divided_day_sum)\n",
    "            #df_offline_divided_day_sum= df_offline_divided_day_sum[['date','delta_s']]\n",
    "            df_sensor_nok_divided_day_sum.columns = ['day', 'nok_'+sensor_name[:-3]]\n",
    "            df_sensor_nok_divided_day_sum = df_sensor_nok_divided_day_sum.set_index(['day'])\n",
    "            #print(df_sensor_nok_divided_day_sum)\n",
    "\n",
    "            df_final = pd.concat([df_final,df_sensor_nok_divided_day_sum],axis=1).fillna(0)\n",
    "            #df_final = pd.merge(df_final, df_sensor_nok_divided_day_sum, how='inner')\n",
    "\n",
    "        print(\"\\tTotal Time Cost: \", round(time.time() - time_start, 3), \"seconds, \",sensor_name)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================S001===================\n",
      "\n",
      "\tLoad File Cost:  16.929 seconds\n",
      "1:  0.884 seconds\n",
      "2:  60.522 seconds\n",
      "3:  60.679 seconds\n",
      "4:  60.681 seconds\n",
      "5:  60.792 seconds\n",
      "\tOffline Process Cost:  60.792 seconds\n",
      "\tTotal Time Cost:  2.427 seconds,  door_contact_as\n",
      "\tTotal Time Cost:  2.637 seconds,  living_room_as\n",
      "\tTotal Time Cost:  4.747 seconds,  bedroom_as\n",
      "\tTotal Time Cost:  3.094 seconds,  bed_as\n",
      "\tTotal Time Cost:  4.319 seconds,  bathroom_as\n",
      "\tTotal Time Cost:  2.631 seconds,  kitchen_as\n",
      "\tTotal Time Cost:  0.147 seconds,  readingroom_as\n",
      "\tTotal Time Cost:  0.161 seconds,  spareroom_as\n"
     ]
    }
   ],
   "source": [
    "idList = list(range(1, 51))\n",
    "\n",
    "for id_ in idList:\n",
    "    all_files = []\n",
    "    print(\"\\n=================={}===================\\n\".format(\"S\"+str(id_).zfill(3)))\n",
    "    for dirpath, dirnames, filenames in os.walk(\"Data\"):\n",
    "        for filename in [f for f in filenames if f.endswith(\"S\"+str(id_).zfill(3)+\".csv\")]:\n",
    "            all_files.append(os.path.join(dirpath, filename))\n",
    "    #load file\n",
    "    df = load_file(all_files)\n",
    "    #process offline record\n",
    "    df_offline,df_final = process_offline(df)\n",
    "    #proess nok record\n",
    "    df_final = proess_nok(df,df_offline,df_final)\n",
    "    df_final.to_csv(\"Output/\"+\"S\"+str(id_).zfill(3)+\".csv\",  encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
